{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nb-title",
   "metadata": {},
   "source": [
    "# Explicabilidad XAI — Scripts 7, 8 y 9\n",
    "\n",
    "**Universidad de Especialidades Espiritu Santo (UEES)**  \n",
    "**Maestria en Inteligencia Artificial — Aprendizaje Automatico**\n",
    "\n",
    "**Estudiantes:**\n",
    "- Ing. Gonzalo Mejia Alcivar\n",
    "- Ing. Jorge Ortiz Merchan\n",
    "- Ing. David Perugachi Rojas\n",
    "\n",
    "---\n",
    "\n",
    "Este notebook integra los tres scripts de explicabilidad XAI del proyecto:\n",
    "\n",
    "| Script | Tecnicas | Imagenes |\n",
    "|--------|----------|----------|\n",
    "| `7_ExplicabilidadSHAP.py` | SHAP + Permutation Feature Importance | 32–39 |\n",
    "| `8_ExplicabilidadPDP_Arbol.py` | PDP + ICE + Arbol de Decision | 40–47 |\n",
    "| `9_VisualizacionesXAI_Integradas.py` | Panel integrado: impacto, comparacion, casos | 48–56 |\n",
    "\n",
    "> **Prerequisito:** ejecutar los scripts 1–6 para generar los modelos `.pkl` en `Models/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nb-s7-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Script 7 — SHAP y Permutation Feature Importance\n",
    "\n",
    "**Tecnicas aplicadas:**\n",
    "- **SHAP (SHapley Additive exPlanations):** `TreeExplainer` sobre Random Forest. Calcula la contribucion de cada feature a cada prediccion usando teoria de juegos cooperativos.\n",
    "- **Permutation Feature Importance (PFI):** mide la caida en F1-Score al permutar aleatoriamente cada feature (5 repeticiones). Aplicado a los 3 modelos.\n",
    "\n",
    "**Resultados clave:**\n",
    "- Feature mas importante (SHAP global): `UtilidadNeta`\n",
    "- Los 3 modelos coinciden en el mismo top-3 con PFI\n",
    "- Imagenes generadas: `32_shap_barplot_global.png` a `39_shap_vs_pfi_comparativa.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams.update({\"figure.max_open_warning\": 0})\n",
    "print(\"Librerias importadas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 1. CONFIGURACION DE RUTAS\n",
    "# =========================================================\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\".\"))\n",
    "# Si ejecutas desde la raiz del proyecto, usar directamente:\n",
    "BASE_DIR = os.path.abspath(\".\")\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"Data\", \"DataSet2024.csv\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"Models\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def save_fig(fig, name):\n",
    "    path = os.path.join(RESULTS_DIR, name)\n",
    "    fig.savefig(path, dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close(fig)\n",
    "    print(f\"  -> Guardado: {path}\")\n",
    "\n",
    "print(f\"BASE_DIR : {BASE_DIR}\")\n",
    "print(f\"DATA_PATH: {DATA_PATH}\")\n",
    "print(f\"MODELS   : {MODELS_DIR}\")\n",
    "print(f\"RESULTS  : {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 2. CARGA DE MODELOS Y DATOS\n",
    "# =========================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"  EXPLICABILIDAD XAI - SHAP Y PERMUTATION IMPORTANCE\")\n",
    "print(\"  Dataset: Empresas del Ecuador - 2024\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n  Cargando modelos exportados...\")\n",
    "dt_model  = joblib.load(os.path.join(MODELS_DIR, \"arbol_decision.pkl\"))\n",
    "rf_model  = joblib.load(os.path.join(MODELS_DIR, \"random_forest.pkl\"))\n",
    "svm_model = joblib.load(os.path.join(MODELS_DIR, \"svm.pkl\"))\n",
    "scaler    = joblib.load(os.path.join(MODELS_DIR, \"scaler.pkl\"))\n",
    "le_target = joblib.load(os.path.join(MODELS_DIR, \"label_encoder_target.pkl\"))\n",
    "le_sector = joblib.load(os.path.join(MODELS_DIR, \"label_encoder_sector.pkl\"))\n",
    "feature_cols = joblib.load(os.path.join(MODELS_DIR, \"feature_columns.pkl\"))\n",
    "\n",
    "clases = list(le_target.classes_)\n",
    "print(f\"  Modelos cargados: Arbol de Decision, SVM, Random Forest\")\n",
    "print(f\"  Features: {feature_cols}\")\n",
    "print(f\"  Clases:   {clases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 3. RECONSTRUCCION DEL CONJUNTO DE PRUEBA\n",
    "# =========================================================\n",
    "print(\"\\n  Reconstruyendo conjunto de prueba...\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, sep=\";\", encoding=\"utf-8-sig\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "df.columns = (\n",
    "    df.columns.str.strip()\n",
    "    .str.replace(\"\\n\", \"_\", regex=False)\n",
    "    .str.replace(\" \", \"_\", regex=False)\n",
    "    .str.replace(\".\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "col_ano = [c for c in df.columns if df[c].nunique() == 1 and df[c].dtype in [\"int64\", \"float64\"]]\n",
    "if col_ano:\n",
    "    df = df.drop(columns=col_ano)\n",
    "\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "for col in df.columns:\n",
    "    if col not in cat_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "epsilon = 1e-7\n",
    "df[\"Margen_Neto\"] = df[\"UtilidadNeta\"] / (df[\"IngresosTotales\"] + epsilon)\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"Margen_Neto\"])\n",
    "df[\"Desempeno\"] = pd.qcut(df[\"Margen_Neto\"], q=3, labels=[\"Bajo\", \"Medio\", \"Alto\"], duplicates=\"drop\")\n",
    "\n",
    "df[\"Sector\"] = le_sector.transform(df[\"Sector\"].astype(str))\n",
    "df[\"Desempeno_cod\"] = le_target.fit_transform(df[\"Desempeno\"])\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[\"Desempeno_cod\"]\n",
    "X_scaled = pd.DataFrame(scaler.transform(X), columns=feature_cols, index=X.index)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "SHAP_SAMPLE = min(2000, len(X_test))\n",
    "X_shap = X_test.sample(n=SHAP_SAMPLE, random_state=42)\n",
    "\n",
    "print(f\"  Conjunto de prueba: {X_test.shape[0]:,} registros\")\n",
    "print(f\"  Muestra para SHAP:  {SHAP_SAMPLE:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-shap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 4. TECNICA 1 — SHAP (SHapley Additive exPlanations)\n",
    "# =========================================================\n",
    "print(\"\\n  Calculando SHAP values para Random Forest...\")\n",
    "\n",
    "explainer_rf = shap.TreeExplainer(rf_model)\n",
    "shap_values_raw = explainer_rf.shap_values(X_shap)\n",
    "\n",
    "# Normalizar estructura: SHAP >= 0.42 retorna array 3D (n_muestras, n_features, n_clases)\n",
    "if isinstance(shap_values_raw, np.ndarray) and shap_values_raw.ndim == 3:\n",
    "    n_clases_shap = shap_values_raw.shape[2]\n",
    "    shap_values_rf = [shap_values_raw[:, :, i] for i in range(n_clases_shap)]\n",
    "elif isinstance(shap_values_raw, list):\n",
    "    shap_values_rf = shap_values_raw\n",
    "else:\n",
    "    shap_values_rf = [shap_values_raw]\n",
    "\n",
    "print(f\"  Shape: {len(shap_values_rf)} clases x {shap_values_rf[0].shape[0]} muestras x {shap_values_rf[0].shape[1]} features\")\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "palette_clase = {\"Alto\": \"#27ae60\", \"Bajo\": \"#e74c3c\", \"Medio\": \"#f39c12\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-shap-barplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1 SHAP Bar Plot: importancia media absoluta global ---\n",
    "print(\"[1/5] SHAP Bar Plot - Importancia media absoluta global...\")\n",
    "\n",
    "mean_abs_shap = np.mean([np.abs(shap_values_rf[i]).mean(axis=0) for i in range(len(clases))], axis=0)\n",
    "shap_importance = pd.DataFrame({\n",
    "    \"Feature\": feature_cols,\n",
    "    \"SHAP_mean_abs\": mean_abs_shap\n",
    "}).sort_values(\"SHAP_mean_abs\", ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "colors_bar = plt.cm.RdYlGn(shap_importance[\"SHAP_mean_abs\"] / shap_importance[\"SHAP_mean_abs\"].max())\n",
    "bars = ax.barh(shap_importance[\"Feature\"], shap_importance[\"SHAP_mean_abs\"],\n",
    "               color=colors_bar, edgecolor=\"black\", linewidth=0.5)\n",
    "for bar, val in zip(bars, shap_importance[\"SHAP_mean_abs\"]):\n",
    "    ax.text(val + shap_importance[\"SHAP_mean_abs\"].max() * 0.01,\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{val:.4f}\", va=\"center\", fontsize=9)\n",
    "ax.set_xlabel(\"Media del valor SHAP absoluto (impacto en la prediccion)\", fontsize=11)\n",
    "ax.set_title(\"SHAP - Importancia Global de Features\\n(Random Forest | Promedio todas las clases)\",\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"32_shap_barplot_global.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-shap-beeswarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2 SHAP Beeswarm Plot por clase ---\n",
    "print(\"[2/5] SHAP Beeswarm Plot por clase...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
    "\n",
    "for i, (clase, ax) in enumerate(zip(clases, axes)):\n",
    "    mean_abs_clase = np.abs(shap_values_rf[i]).mean(axis=0)\n",
    "    order_idx = np.argsort(mean_abs_clase)[::-1]\n",
    "    top_n = min(8, len(feature_cols))\n",
    "    top_idx = order_idx[:top_n]\n",
    "\n",
    "    shap_vals_clase = shap_values_rf[i][:, top_idx]\n",
    "    feat_vals_clase = X_shap.iloc[:, top_idx]\n",
    "    feat_names_top  = [feature_cols[j] for j in top_idx]\n",
    "\n",
    "    for k, feat_name in enumerate(feat_names_top):\n",
    "        y_pos   = top_n - 1 - k\n",
    "        sv      = shap_vals_clase[:, k]\n",
    "        fv      = feat_vals_clase.iloc[:, k].values\n",
    "        fv_norm = (fv - fv.min()) / (fv.max() - fv.min() + 1e-9)\n",
    "        colors  = plt.cm.coolwarm(fv_norm)\n",
    "        jitter  = np.random.RandomState(42).uniform(-0.2, 0.2, len(sv))\n",
    "        ax.scatter(sv, y_pos + jitter, c=colors, alpha=0.4, s=8, linewidths=0)\n",
    "\n",
    "    ax.axvline(x=0, color=\"black\", linewidth=0.8, linestyle=\"--\")\n",
    "    ax.set_yticks(range(top_n))\n",
    "    ax.set_yticklabels(feat_names_top[::-1], fontsize=9)\n",
    "    ax.set_xlabel(\"Valor SHAP\", fontsize=10)\n",
    "    ax.set_title(f\"Clase: {clase}\", fontsize=12, fontweight=\"bold\",\n",
    "                 color=palette_clase[clase])\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"SHAP Beeswarm - Impacto de Features por Clase\\n\"\n",
    "             \"(Azul = valor bajo, Rojo = valor alto de la feature)\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "sm   = plt.cm.ScalarMappable(cmap=\"coolwarm\", norm=plt.Normalize(0, 1))\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=axes, orientation=\"vertical\", fraction=0.02, pad=0.04)\n",
    "cbar.set_label(\"Valor de la feature (normalizado)\", fontsize=10)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"33_shap_beeswarm_por_clase.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-shap-waterfall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 SHAP Waterfall Plot individual ---\n",
    "print(\"[3/5] SHAP Waterfall Plot - Explicacion individual...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
    "y_shap_pred = rf_model.predict(X_shap)\n",
    "\n",
    "for i, (clase_idx, clase_nombre) in enumerate(enumerate(clases)):\n",
    "    ax   = axes[i]\n",
    "    mask = y_shap_pred == clase_idx\n",
    "    if mask.sum() == 0:\n",
    "        ax.text(0.5, 0.5, f\"Sin muestras\\npredichas como\\n{clase_nombre}\",\n",
    "                ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title(f\"Clase: {clase_nombre}\", fontsize=12)\n",
    "        continue\n",
    "\n",
    "    sample_idx = np.where(mask)[0][0]\n",
    "    sv         = shap_values_rf[clase_idx][sample_idx]\n",
    "    ev         = explainer_rf.expected_value\n",
    "    base_val   = ev[clase_idx] if hasattr(ev, \"__len__\") else float(ev)\n",
    "\n",
    "    order      = np.argsort(np.abs(sv))[::-1]\n",
    "    top_k      = min(8, len(sv))\n",
    "    sv_top     = sv[order[:top_k]]\n",
    "    fn_top     = [feature_cols[j] for j in order[:top_k]]\n",
    "    cumsum     = np.cumsum(sv_top)\n",
    "    starts     = np.concatenate([[base_val], base_val + cumsum[:-1]])\n",
    "    colors_wf  = [\"#e74c3c\" if v > 0 else \"#3498db\" for v in sv_top]\n",
    "\n",
    "    ax.barh(range(top_k), sv_top, left=starts, color=colors_wf,\n",
    "            edgecolor=\"black\", linewidth=0.5, height=0.6)\n",
    "    ax.axvline(x=base_val, color=\"gray\", linewidth=1, linestyle=\"--\",\n",
    "               alpha=0.7, label=f\"Base: {base_val:.3f}\")\n",
    "    ax.axvline(x=base_val + cumsum[-1], color=\"black\", linewidth=1.5,\n",
    "               label=f\"Pred: {base_val + cumsum[-1]:.3f}\")\n",
    "    ax.set_yticks(range(top_k))\n",
    "    ax.set_yticklabels(fn_top, fontsize=9)\n",
    "    ax.set_xlabel(\"Contribucion SHAP\", fontsize=10)\n",
    "    ax.set_title(f\"Prediccion: {clase_nombre}\\n(Rojo = aumenta prob, Azul = reduce prob)\",\n",
    "                 fontsize=11, fontweight=\"bold\", color=palette_clase[clase_nombre])\n",
    "    ax.legend(fontsize=8, loc=\"lower right\")\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"SHAP Waterfall - Explicacion de Predicciones Individuales\\n(Top 8 features por clase)\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"34_shap_waterfall_individual.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-shap-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.4 SHAP Dependence Plot ---\n",
    "print(\"[4/5] SHAP Dependence Plot - UtilidadNeta...\")\n",
    "\n",
    "top_feature        = shap_importance.iloc[-1][\"Feature\"]\n",
    "top_feature_idx    = feature_cols.index(top_feature)\n",
    "second_feature     = shap_importance.iloc[-2][\"Feature\"]\n",
    "second_feature_idx = feature_cols.index(second_feature)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for i, (clase_nombre, ax) in enumerate(zip(clases, axes)):\n",
    "    sv_feat    = shap_values_rf[i][:, top_feature_idx]\n",
    "    feat_vals  = X_shap.iloc[:, top_feature_idx].values\n",
    "    color_vals = X_shap.iloc[:, second_feature_idx].values\n",
    "\n",
    "    sc = ax.scatter(feat_vals, sv_feat, c=color_vals, cmap=\"viridis\",\n",
    "                    alpha=0.5, s=15, linewidths=0)\n",
    "    plt.colorbar(sc, ax=ax, label=second_feature, shrink=0.8)\n",
    "    ax.axhline(y=0, color=\"black\", linewidth=0.8, linestyle=\"--\")\n",
    "\n",
    "    z = np.polyfit(feat_vals, sv_feat, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(feat_vals.min(), feat_vals.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), \"r--\", linewidth=1.5, alpha=0.8, label=\"Tendencia\")\n",
    "\n",
    "    ax.set_xlabel(f\"{top_feature} (escalado)\", fontsize=10)\n",
    "    ax.set_ylabel(f\"SHAP value ({clase_nombre})\", fontsize=10)\n",
    "    ax.set_title(f\"Clase: {clase_nombre}\", fontsize=12, fontweight=\"bold\",\n",
    "                 color=palette_clase[clase_nombre])\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(f\"SHAP Dependence Plot: {top_feature}\\n(Coloreado por {second_feature})\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"35_shap_dependence_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-shap-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.5 SHAP Heatmap por clase ---\n",
    "print(\"[5/5] SHAP Heatmap - Resumen de importancias por clase...\")\n",
    "\n",
    "shap_por_clase = pd.DataFrame(\n",
    "    {clase: np.abs(shap_values_rf[i]).mean(axis=0) for i, clase in enumerate(clases)},\n",
    "    index=feature_cols\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(shap_por_clase, annot=True, fmt=\".4f\", cmap=\"YlOrRd\",\n",
    "            linewidths=0.5, ax=ax, cbar_kws={\"label\": \"SHAP media absoluta\"})\n",
    "ax.set_title(\"SHAP - Importancia Media Absoluta por Feature y Clase\\n(Random Forest)\",\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Clase de Desempeno\", fontsize=11)\n",
    "ax.set_ylabel(\"Feature\", fontsize=11)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"36_shap_heatmap_por_clase.png\")\n",
    "\n",
    "print(f\"\\n  Feature mas importante (SHAP): {shap_importance.iloc[-1]['Feature']}\")\n",
    "print(f\"  SHAP medio absoluto: {shap_importance.iloc[-1]['SHAP_mean_abs']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-pfi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 5. TECNICA 2 — PERMUTATION FEATURE IMPORTANCE\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  TECNICA 2: PERMUTATION FEATURE IMPORTANCE\")\n",
    "print(\"  Modelos: Arbol de Decision, SVM, Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "modelos_pfi = {\n",
    "    \"Arbol de Decision\": dt_model,\n",
    "    \"SVM\":               svm_model,\n",
    "    \"Random Forest\":     rf_model,\n",
    "}\n",
    "colores_modelo = {\n",
    "    \"Arbol de Decision\": \"#3498db\",\n",
    "    \"SVM\":               \"#e67e22\",\n",
    "    \"Random Forest\":     \"#27ae60\",\n",
    "}\n",
    "\n",
    "PFI_SAMPLE = min(5000, len(X_test))\n",
    "X_pfi = X_test.sample(n=PFI_SAMPLE, random_state=42)\n",
    "y_pfi = y_test.loc[X_pfi.index]\n",
    "\n",
    "print(f\"\\n  Muestra para PFI: {PFI_SAMPLE:,} registros | Repeticiones: 5\")\n",
    "\n",
    "resultados_pfi = {}\n",
    "for nombre, modelo in modelos_pfi.items():\n",
    "    print(f\"\\n  Calculando PFI para {nombre}...\")\n",
    "    pfi = permutation_importance(\n",
    "        modelo, X_pfi, y_pfi,\n",
    "        n_repeats=5, random_state=42,\n",
    "        scoring=\"f1_weighted\", n_jobs=-1\n",
    "    )\n",
    "    resultados_pfi[nombre] = {\n",
    "        \"mean\": pfi.importances_mean,\n",
    "        \"std\":  pfi.importances_std,\n",
    "        \"raw\":  pfi.importances,\n",
    "    }\n",
    "    top3_idx = np.argsort(pfi.importances_mean)[::-1][:3]\n",
    "    print(f\"    Top 3: \" +\n",
    "          \", \".join([f\"{feature_cols[i]} ({pfi.importances_mean[i]:.4f})\" for i in top3_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-pfi-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 Barplot comparativo de PFI ---\n",
    "print(\"[1/3] Barplot comparativo de PFI por modelo...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
    "for ax, (nombre, res) in zip(axes, resultados_pfi.items()):\n",
    "    orden         = np.argsort(res[\"mean\"])\n",
    "    feat_sorted   = [feature_cols[i] for i in orden]\n",
    "    means_sorted  = res[\"mean\"][orden]\n",
    "    stds_sorted   = res[\"std\"][orden]\n",
    "    colors_pfi    = [colores_modelo[nombre] if m > 0 else \"#bdc3c7\" for m in means_sorted]\n",
    "\n",
    "    bars = ax.barh(feat_sorted, means_sorted, xerr=stds_sorted,\n",
    "                   color=colors_pfi, edgecolor=\"black\", linewidth=0.4,\n",
    "                   error_kw={\"elinewidth\": 1.5, \"capsize\": 4, \"ecolor\": \"black\"}, height=0.6)\n",
    "    ax.axvline(x=0, color=\"black\", linewidth=0.8, linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Disminucion en F1-Score (weighted)\", fontsize=10)\n",
    "    ax.set_title(nombre, fontsize=12, fontweight=\"bold\", color=colores_modelo[nombre])\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.4)\n",
    "    for bar, val, std in zip(bars, means_sorted, stds_sorted):\n",
    "        if val > 0:\n",
    "            ax.text(val + std + max(means_sorted) * 0.01,\n",
    "                    bar.get_y() + bar.get_height() / 2,\n",
    "                    f\"{val:.4f}\", va=\"center\", fontsize=8)\n",
    "\n",
    "fig.suptitle(\"Permutation Feature Importance por Modelo\\n\"\n",
    "             \"(Barras = media ± std sobre 5 repeticiones)\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"37_pfi_barplot_por_modelo.png\")\n",
    "\n",
    "# --- 5.2 Heatmap comparativo ---\n",
    "print(\"[2/3] Heatmap comparativo de PFI (3 modelos)...\")\n",
    "pfi_df = pd.DataFrame(\n",
    "    {nombre: res[\"mean\"] for nombre, res in resultados_pfi.items()},\n",
    "    index=feature_cols\n",
    ").sort_values(\"Random Forest\", ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(pfi_df, annot=True, fmt=\".4f\", cmap=\"RdYlGn\",\n",
    "            center=0, linewidths=0.5, ax=ax,\n",
    "            cbar_kws={\"label\": \"Disminucion media en F1-Score\"})\n",
    "ax.set_title(\"Permutation Feature Importance - Comparativa de Modelos\",\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"38_pfi_heatmap_comparativo.png\")\n",
    "\n",
    "# --- 5.3 SHAP vs PFI ---\n",
    "print(\"[3/3] Comparativa SHAP vs PFI para Random Forest...\")\n",
    "shap_imp_rf = pd.Series(\n",
    "    sum(np.abs(shap_values_rf[i]).mean(axis=0) for i in range(len(clases))),\n",
    "    index=feature_cols\n",
    ")\n",
    "pfi_imp_rf = pd.Series(resultados_pfi[\"Random Forest\"][\"mean\"], index=feature_cols)\n",
    "shap_norm  = (shap_imp_rf - shap_imp_rf.min()) / (shap_imp_rf.max() - shap_imp_rf.min())\n",
    "pfi_norm   = (pfi_imp_rf  - pfi_imp_rf.min())  / (pfi_imp_rf.max()  - pfi_imp_rf.min())\n",
    "\n",
    "comp_df = pd.DataFrame({\n",
    "    \"SHAP (normalizado)\": shap_norm,\n",
    "    \"PFI (normalizado)\":  pfi_norm,\n",
    "}).sort_values(\"SHAP (normalizado)\", ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "x     = np.arange(len(comp_df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, comp_df[\"SHAP (normalizado)\"], width,\n",
    "       label=\"SHAP\", color=\"#9b59b6\", edgecolor=\"black\", linewidth=0.5, alpha=0.85)\n",
    "ax.bar(x + width/2, comp_df[\"PFI (normalizado)\"],  width,\n",
    "       label=\"Permutation Feature Importance\", color=\"#27ae60\",\n",
    "       edgecolor=\"black\", linewidth=0.5, alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comp_df.index, rotation=30, ha=\"right\", fontsize=10)\n",
    "ax.set_ylabel(\"Importancia normalizada [0, 1]\", fontsize=11)\n",
    "ax.set_title(\"Comparativa SHAP vs Permutation Feature Importance\\n(Random Forest)\",\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0, 1.2)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"39_shap_vs_pfi_comparativa.png\")\n",
    "\n",
    "print(\"\\n  Script 7 completado. Imagenes 32-39 generadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nb-s8-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Script 8 — PDP, ICE y Visualizacion del Arbol de Decision\n",
    "\n",
    "**Tecnicas aplicadas:**\n",
    "- **Partial Dependence Plots (PDP):** relacion marginal entre cada feature y la probabilidad predicha, manteniendo las demas constantes en su valor promedio. Aplicado a las 4 features mas importantes x 3 clases.\n",
    "- **ICE (Individual Conditional Expectation):** 300 curvas PDP individuales sobre `UtilidadNeta`. Permite detectar heterogeneidad en el efecto de la variable.\n",
    "- **Visualizacion del Arbol de Decision:** estructura grafica (4 niveles), tabla de nodos, frecuencia de features y analisis de profundidad vs F1-Score.\n",
    "\n",
    "**Resultados clave:**\n",
    "- Top 4 features (Gini RF): `UtilidadNeta`, `UtilidadEjercicio`, `IngresosTotales`, `IngresoVentas`\n",
    "- Profundidad actual del arbol: 10 | Hojas: 246 | Profundidad optima (F1 max): 19 (F1=0.9914)\n",
    "- Feature mas usada en nodos: `UtilidadNeta` (96 veces)\n",
    "- Imagenes generadas: `40_pdp_top4_features_por_clase.png` a `47_comparativa_tecnicas_xai.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SCRIPT 8 — Imports adicionales\n",
    "# =========================================================\n",
    "from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Reconstruir datos de entrenamiento (necesarios para el analisis del arbol)\n",
    "X_train, X_test2, y_train, y_test2 = train_test_split(\n",
    "    X_scaled, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "PDP_SAMPLE = min(3000, len(X_test2))\n",
    "X_pdp = X_test2.sample(n=PDP_SAMPLE, random_state=42)\n",
    "y_pdp = y_test2.loc[X_pdp.index]\n",
    "\n",
    "importancias_rf = rf_model.feature_importances_\n",
    "feat_imp_order  = np.argsort(importancias_rf)[::-1]\n",
    "top4_features   = [feature_cols[i] for i in feat_imp_order[:4]]\n",
    "top4_idx        = feat_imp_order[:4].tolist()\n",
    "top1_feature    = top4_features[0]\n",
    "top1_idx        = top4_idx[0]\n",
    "top2_feature    = top4_features[1]\n",
    "top2_idx        = top4_idx[1]\n",
    "\n",
    "idx_alto  = clases.index(\"Alto\")\n",
    "idx_bajo  = clases.index(\"Bajo\")\n",
    "idx_medio = clases.index(\"Medio\")\n",
    "\n",
    "print(f\"  Top 4 features (Gini RF): {top4_features}\")\n",
    "print(f\"  Muestra PDP: {PDP_SAMPLE:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-pdp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# TECNICA 3 — PARTIAL DEPENDENCE PLOTS (PDP)\n",
    "# =========================================================\n",
    "\n",
    "# --- PDP top 4 features x 3 clases ---\n",
    "print(\"[1/4] PDP de las 4 features mas importantes (3 clases)...\")\n",
    "fig, axes = plt.subplots(3, 4, figsize=(22, 16))\n",
    "\n",
    "for row, (clase_nombre, clase_idx) in enumerate(zip(clases, [idx_alto, idx_bajo, idx_medio])):\n",
    "    for col, (feat_nombre, feat_idx) in enumerate(zip(top4_features, top4_idx)):\n",
    "        ax        = axes[row, col]\n",
    "        pd_result = partial_dependence(rf_model, X_pdp, features=[feat_idx],\n",
    "                                       kind=\"average\", grid_resolution=50)\n",
    "        grid_vals = pd_result[\"grid_values\"][0]\n",
    "        avg_raw   = pd_result[\"average\"]\n",
    "        avg_pred  = avg_raw[clase_idx] if avg_raw.ndim == 2 else avg_raw[0]\n",
    "\n",
    "        ax.plot(grid_vals, avg_pred, color=palette_clase[clase_nombre], linewidth=2.5)\n",
    "        ax.fill_between(grid_vals, avg_pred, alpha=0.15, color=palette_clase[clase_nombre])\n",
    "        ax.axhline(y=avg_pred.mean(), color=\"gray\", linewidth=1, linestyle=\"--\", alpha=0.7)\n",
    "        ax.set_xlabel(f\"{feat_nombre} (escalado)\", fontsize=9)\n",
    "        ax.set_ylabel(\"Probabilidad\" if col == 0 else \"\", fontsize=9)\n",
    "        ax.set_title(f\"{feat_nombre}\\nClase: {clase_nombre}\",\n",
    "                     fontsize=10, fontweight=\"bold\", color=palette_clase[clase_nombre])\n",
    "        ax.grid(linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "fig.suptitle(\"Partial Dependence Plots (PDP) - Top 4 Features x 3 Clases\\n(Random Forest)\",\n",
    "             fontsize=15, fontweight=\"bold\", y=1.01)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"40_pdp_top4_features_por_clase.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-pdp2d-ice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PDP bidimensional ---\n",
    "print(\"[2/4] PDP bidimensional - interaccion entre top 2 features...\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "for ax, (clase_nombre, clase_idx) in zip(axes, zip(clases, [idx_alto, idx_bajo, idx_medio])):\n",
    "    pd_2d   = partial_dependence(rf_model, X_pdp, features=[(top1_idx, top2_idx)],\n",
    "                                 kind=\"average\", grid_resolution=20)\n",
    "    avg_2d  = pd_2d[\"average\"]\n",
    "    Z       = avg_2d[clase_idx] if avg_2d.ndim == 3 else avg_2d[0]\n",
    "    xx, yy  = pd_2d[\"grid_values\"][0], pd_2d[\"grid_values\"][1]\n",
    "    XX, YY  = np.meshgrid(xx, yy)\n",
    "    im      = ax.contourf(XX, YY, Z.T, levels=20, cmap=\"RdYlGn\")\n",
    "    ax.contour(XX, YY, Z.T, levels=10, colors=\"black\", linewidths=0.3, alpha=0.4)\n",
    "    plt.colorbar(im, ax=ax, label=\"Probabilidad predicha\")\n",
    "    ax.set_xlabel(f\"{top1_feature} (escalado)\", fontsize=10)\n",
    "    ax.set_ylabel(f\"{top2_feature} (escalado)\", fontsize=10)\n",
    "    ax.set_title(f\"Clase: {clase_nombre}\", fontsize=12, fontweight=\"bold\",\n",
    "                 color=palette_clase[clase_nombre])\n",
    "fig.suptitle(f\"PDP Bidimensional: {top1_feature} x {top2_feature}\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"41_pdp_bidimensional_interaccion.png\")\n",
    "\n",
    "# --- ICE Plot ---\n",
    "print(\"[3/4] ICE Plot - Individual Conditional Expectation...\")\n",
    "ICE_SAMPLE = min(300, len(X_pdp))\n",
    "X_ice      = X_pdp.sample(n=ICE_SAMPLE, random_state=42)\n",
    "fig, axes  = plt.subplots(1, 3, figsize=(20, 6))\n",
    "for ax, (clase_nombre, clase_idx) in zip(axes, zip(clases, [idx_alto, idx_bajo, idx_medio])):\n",
    "    pd_ice    = partial_dependence(rf_model, X_ice, features=[top1_idx],\n",
    "                                   kind=\"both\", grid_resolution=40)\n",
    "    grid_vals = pd_ice[\"grid_values\"][0]\n",
    "    ind_raw   = pd_ice[\"individual\"]\n",
    "    avg_raw   = pd_ice[\"average\"]\n",
    "    ice_lines = ind_raw[clase_idx] if ind_raw.ndim == 3 else ind_raw[0]\n",
    "    avg_line  = avg_raw[clase_idx] if avg_raw.ndim == 2 else avg_raw[0]\n",
    "    for line in ice_lines:\n",
    "        ax.plot(grid_vals, line, color=palette_clase[clase_nombre], linewidth=0.4, alpha=0.15)\n",
    "    ax.plot(grid_vals, avg_line, color=\"black\", linewidth=2.5, label=\"PDP (promedio)\", zorder=5)\n",
    "    ax.set_xlabel(f\"{top1_feature} (escalado)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Probabilidad predicha\" if ax == axes[0] else \"\", fontsize=10)\n",
    "    ax.set_title(f\"Clase: {clase_nombre}\\n({ICE_SAMPLE} curvas ICE)\",\n",
    "                 fontsize=11, fontweight=\"bold\", color=palette_clase[clase_nombre])\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(linestyle=\"--\", alpha=0.4)\n",
    "fig.suptitle(f\"ICE Plot: {top1_feature}\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"42_ice_plot_top_feature.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-tree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# TECNICA 4 — VISUALIZACION DEL ARBOL DE DECISION\n",
    "# =========================================================\n",
    "\n",
    "# --- Arbol grafico (4 niveles) ---\n",
    "print(\"[1/4] Arbol de Decision detallado (profundidad 4)...\")\n",
    "fig, ax = plt.subplots(figsize=(28, 14))\n",
    "plot_tree(dt_model, max_depth=4, feature_names=feature_cols, class_names=clases,\n",
    "          filled=True, rounded=True, fontsize=7, ax=ax, impurity=True, precision=3)\n",
    "ax.set_title(\"Arbol de Decision - Estructura Detallada (primeros 4 niveles)\",\n",
    "             fontsize=14, fontweight=\"bold\")\n",
    "save_fig(fig, \"44_arbol_decision_detallado.png\")\n",
    "\n",
    "# --- Analisis de nodos ---\n",
    "print(\"[2/4] Analisis de nodos internos...\")\n",
    "tree_              = dt_model.tree_\n",
    "feature_names_arr  = np.array(feature_cols)\n",
    "nodos_internos     = []\n",
    "for node_id in range(tree_.node_count):\n",
    "    if tree_.feature[node_id] >= 0:\n",
    "        nodos_internos.append({\n",
    "            \"Nodo\":       node_id,\n",
    "            \"Feature\":    feature_names_arr[tree_.feature[node_id]],\n",
    "            \"Umbral\":     round(tree_.threshold[node_id], 4),\n",
    "            \"Muestras\":   tree_.n_node_samples[node_id],\n",
    "            \"Gini\":       round(tree_.impurity[node_id], 4),\n",
    "            \"Profundidad\": int(np.floor(np.log2(node_id + 1))) if node_id > 0 else 0,\n",
    "        })\n",
    "df_nodos       = pd.DataFrame(nodos_internos).head(20)\n",
    "feature_freq   = pd.Series(feature_names_arr[tree_.feature[tree_.feature >= 0]]).value_counts()\n",
    "print(f\"  Feature mas usada: {feature_freq.index[0]} ({feature_freq.iloc[0]} veces)\")\n",
    "print(df_nodos.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-depth-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Profundidad vs accuracy ---\n",
    "print(\"[3/4] Analisis profundidad vs accuracy...\")\n",
    "profundidades = list(range(1, 21))\n",
    "train_accs, test_accs, train_f1s, test_f1s = [], [], [], []\n",
    "\n",
    "for prof in profundidades:\n",
    "    dt_temp = DecisionTreeClassifier(max_depth=prof, min_samples_split=20,\n",
    "                                      min_samples_leaf=10, class_weight=\"balanced\",\n",
    "                                      random_state=42)\n",
    "    dt_temp.fit(X_train, y_train)\n",
    "    y_tr_pred = dt_temp.predict(X_train)\n",
    "    y_te_pred = dt_temp.predict(X_test2)\n",
    "    train_accs.append(accuracy_score(y_train, y_tr_pred))\n",
    "    test_accs.append(accuracy_score(y_test2, y_te_pred))\n",
    "    train_f1s.append(f1_score(y_train, y_tr_pred, average=\"weighted\"))\n",
    "    test_f1s.append(f1_score(y_test2, y_te_pred, average=\"weighted\"))\n",
    "\n",
    "opt_prof = profundidades[np.argmax(test_f1s)]\n",
    "print(f\"  Profundidad optima (F1 max en prueba): {opt_prof} | F1 = {max(test_f1s):.4f}\")\n",
    "print(f\"  Profundidad actual del modelo: {dt_model.get_depth()}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "for ax, (metric, tr, te, ylabel) in zip(axes, [\n",
    "    (\"Accuracy\", train_accs, test_accs, \"Accuracy\"),\n",
    "    (\"F1-Score\", train_f1s, test_f1s,  \"F1-Score (weighted)\")\n",
    "]):\n",
    "    ax.plot(profundidades, tr, \"o-\", color=\"#3498db\", linewidth=2, markersize=6, label=\"Entrenamiento\")\n",
    "    ax.plot(profundidades, te, \"s-\", color=\"#e74c3c\", linewidth=2, markersize=6, label=\"Prueba\")\n",
    "    ax.axvline(x=dt_model.get_depth(), color=\"green\", linewidth=1.5, linestyle=\"--\",\n",
    "               label=f\"Actual ({dt_model.get_depth()})\")\n",
    "    if metric == \"F1-Score\":\n",
    "        ax.axvline(x=opt_prof, color=\"orange\", linewidth=1.5, linestyle=\":\",\n",
    "                   label=f\"Optima ({opt_prof})\")\n",
    "    ax.set_xlabel(\"Profundidad maxima\", fontsize=11)\n",
    "    ax.set_ylabel(ylabel, fontsize=11)\n",
    "    ax.set_title(f\"{metric} vs Profundidad\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim(0.5, 1.02)\n",
    "    ax.grid(linestyle=\"--\", alpha=0.4)\n",
    "fig.suptitle(\"Efecto de la Profundidad del Arbol en el Rendimiento\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"46_arbol_profundidad_vs_accuracy.png\")\n",
    "\n",
    "print(\"\\n  Script 8 completado. Imagenes 40-47 generadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nb-s9-title",
   "metadata": {},
   "source": [
    "---\n",
    "## Script 9 — Visualizaciones XAI Integradas\n",
    "\n",
    "**Tres bloques de analisis:**\n",
    "\n",
    "| Bloque | Objetivo | Imagenes |\n",
    "|--------|----------|----------|\n",
    "| 1 — Variables con mayor impacto | Panel 4 tecnicas, ranking consenso, heatmap 5 tecnicas | 48–50 |\n",
    "| 2 — Comparacion de explicaciones | Concordancia SHAP/PFI/Gini, PDP 3 modelos, tabla resumen | 51–53 |\n",
    "| 3 — Casos individuales | Waterfall 2 empresas, radar financiero, panel 2×3 | 54–56 |\n",
    "\n",
    "**Resultados de ejecucion:**\n",
    "- Top 3 por consenso (5 tecnicas): `UtilidadNeta`, `IngresosTotales`, `UtilidadEjercicio`\n",
    "- Caso 1 (Alto, idx=36634): P(Alto)=100.00% — feature decisiva: `UtilidadNeta`\n",
    "- Caso 2 (Bajo, idx=106402): P(Bajo)=98.96% — feature decisiva: `UtilidadNeta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SCRIPT 9 — Setup: importancias de las 5 tecnicas\n",
    "# =========================================================\n",
    "IMG_START = 48\n",
    "\n",
    "TECNICAS_COLORS = {\n",
    "    \"SHAP\": \"#9b59b6\", \"PFI\": \"#27ae60\",\n",
    "    \"Gini RF\": \"#3498db\", \"Gini DT\": \"#e67e22\", \"PDP Range\": \"#e74c3c\",\n",
    "}\n",
    "\n",
    "# Gini\n",
    "imp_gini_dt = pd.Series(dt_model.feature_importances_, index=feature_cols)\n",
    "imp_gini_rf = pd.Series(rf_model.feature_importances_, index=feature_cols)\n",
    "\n",
    "# PFI sobre muestra XAI\n",
    "X_sample = X_test.sample(n=min(2000, len(X_test)), random_state=42)\n",
    "y_sample = y_test.loc[X_sample.index]\n",
    "pfi_res  = permutation_importance(rf_model, X_sample, y_sample,\n",
    "                                   n_repeats=5, random_state=42,\n",
    "                                   scoring=\"f1_weighted\", n_jobs=-1)\n",
    "imp_pfi  = pd.Series(pfi_res.importances_mean, index=feature_cols)\n",
    "\n",
    "# SHAP (reutilizar explainer_rf y shap_values_rf del script 7)\n",
    "imp_shap = pd.Series(\n",
    "    np.mean([np.abs(shap_values_rf[i]).mean(axis=0) for i in range(len(clases))], axis=0),\n",
    "    index=feature_cols\n",
    ")\n",
    "\n",
    "# PDP range top 4\n",
    "imp_pdp = {}\n",
    "for fi, fn in zip(top4_idx, top4_features):\n",
    "    try:\n",
    "        pd_r    = partial_dependence(rf_model, X_sample, features=[fi],\n",
    "                                     kind=\"average\", grid_resolution=30)\n",
    "        avg_r   = pd_r[\"average\"]\n",
    "        avg_alt = avg_r[idx_alto] if avg_r.ndim == 2 else avg_r[0]\n",
    "        imp_pdp[fn] = float(avg_alt.max() - avg_alt.min())\n",
    "    except Exception:\n",
    "        imp_pdp[fn] = 0.0\n",
    "for f in feature_cols:\n",
    "    if f not in imp_pdp: imp_pdp[f] = 0.0\n",
    "imp_pdp_s = pd.Series(imp_pdp)\n",
    "\n",
    "def norm01(s):\n",
    "    mn, mx = s.min(), s.max()\n",
    "    return (s - mn) / (mx - mn + 1e-9)\n",
    "\n",
    "n_shap    = norm01(imp_shap)\n",
    "n_pfi     = norm01(imp_pfi.clip(lower=0))\n",
    "n_gini_rf = norm01(imp_gini_rf)\n",
    "n_gini_dt = norm01(imp_gini_dt)\n",
    "n_pdp     = norm01(imp_pdp_s)\n",
    "order_shap = n_shap.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "print(f\"  Top 3 (SHAP):    {order_shap[:3]}\")\n",
    "print(f\"  Top 3 (Gini RF): {imp_gini_rf.sort_values(ascending=False).index[:3].tolist()}\")\n",
    "print(f\"  Top 3 (PFI):     {imp_pfi.sort_values(ascending=False).index[:3].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9-bloque1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# BLOQUE 1 — VARIABLES CON MAYOR IMPACTO\n",
    "# =========================================================\n",
    "\n",
    "# 48: Panel consolidado 4 tecnicas\n",
    "comp_df = pd.DataFrame({\"SHAP\": n_shap, \"PFI\": n_pfi,\n",
    "                         \"Gini RF\": n_gini_rf, \"Gini DT\": n_gini_dt},\n",
    "                        index=feature_cols).loc[order_shap]\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "x       = np.arange(len(comp_df))\n",
    "width   = 0.20\n",
    "offsets = [-1.5, -0.5, 0.5, 1.5]\n",
    "cols    = [\"SHAP\", \"PFI\", \"Gini RF\", \"Gini DT\"]\n",
    "for off, col, clr in zip(offsets, cols, [TECNICAS_COLORS[c] for c in cols]):\n",
    "    ax.bar(x + off * width, comp_df[col], width, label=col, color=clr,\n",
    "           edgecolor=\"black\", linewidth=0.4, alpha=0.88)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comp_df.index, rotation=30, ha=\"right\", fontsize=10)\n",
    "ax.set_ylabel(\"Importancia normalizada [0, 1]\", fontsize=11)\n",
    "ax.set_title(\"Variables con Mayor Impacto en el Modelo\\n(4 tecnicas XAI | normalizadas a [0,1])\",\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "ax.legend(fontsize=10, loc=\"upper right\")\n",
    "ax.set_ylim(0, 1.25)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, f\"{IMG_START}_impacto_panel_consolidado.png\")\n",
    "\n",
    "# 49: Ranking consenso\n",
    "ranking_final = pd.DataFrame({\"SHAP\": n_shap, \"PFI\": n_pfi,\n",
    "                               \"Gini RF\": n_gini_rf, \"Gini DT\": n_gini_dt,\n",
    "                               \"PDP\": n_pdp}).mean(axis=1).sort_values(ascending=False)\n",
    "print(f\"\\n  Top 3 por consenso (5 tecnicas): {ranking_final.index[:3].tolist()}\")\n",
    "\n",
    "# 50: Heatmap 5 tecnicas\n",
    "heatmap_df = pd.DataFrame({\"SHAP\": n_shap, \"PFI\": n_pfi, \"Gini RF\": n_gini_rf,\n",
    "                            \"Gini DT\": n_gini_dt, \"PDP Range\": n_pdp},\n",
    "                           index=feature_cols).loc[order_shap]\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt=\".3f\", cmap=\"YlOrRd\",\n",
    "            linewidths=0.5, ax=ax, vmin=0, vmax=1,\n",
    "            cbar_kws={\"label\": \"Importancia normalizada [0, 1]\"})\n",
    "ax.set_title(\"Mapa de Calor de Importancia: 5 Tecnicas XAI x 10 Features\",\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "fig.tight_layout()\n",
    "save_fig(fig, f\"{IMG_START + 2}_impacto_heatmap_5tecnicas.png\")\n",
    "\n",
    "print(\"  Bloque 1 completado (imagenes 48-50).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9-bloque2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# BLOQUE 2 — COMPARACION DE EXPLICACIONES\n",
    "# =========================================================\n",
    "n_feat = len(feature_cols)\n",
    "\n",
    "# 51: Dispersion SHAP vs PFI vs Gini RF\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "pares = [(\"SHAP\", \"PFI\", n_shap, n_pfi),\n",
    "         (\"SHAP\", \"Gini RF\", n_shap, n_gini_rf),\n",
    "         (\"PFI\",  \"Gini RF\", n_pfi,  n_gini_rf)]\n",
    "\n",
    "for ax, (lbl_x, lbl_y, sx, sy) in zip(axes, pares):\n",
    "    ax.scatter(sx.values, sy.values, s=120, c=range(n_feat),\n",
    "               cmap=\"tab10\", edgecolors=\"black\", linewidths=0.6, zorder=3)\n",
    "    for feat, vx, vy in zip(feature_cols, sx.values, sy.values):\n",
    "        ax.annotate(feat, (vx, vy), textcoords=\"offset points\",\n",
    "                    xytext=(5, 4), fontsize=7.5, color=\"#2c3e50\")\n",
    "    lim  = max(sx.max(), sy.max()) * 1.05\n",
    "    ax.plot([0, lim], [0, lim], \"r--\", linewidth=1.2, alpha=0.7, label=\"Concordancia perfecta\")\n",
    "    corr = pd.Series(sx.values).corr(pd.Series(sy.values), method=\"spearman\")\n",
    "    ax.set_xlabel(f\"{lbl_x} (normalizado)\", fontsize=10)\n",
    "    ax.set_ylabel(f\"{lbl_y} (normalizado)\", fontsize=10)\n",
    "    ax.set_title(f\"{lbl_x} vs {lbl_y}\\nr_spearman = {corr:.3f}\",\n",
    "                 fontsize=11, fontweight=\"bold\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "fig.suptitle(\"Concordancia entre Tecnicas XAI\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, f\"{IMG_START + 3}_comparacion_concordancia.png\")\n",
    "\n",
    "print(\"  Bloque 2 completado (imagenes 51-53).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9-bloque3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# BLOQUE 3 — CASOS INDIVIDUALES CON DECISIONES EXPLICADAS\n",
    "# =========================================================\n",
    "y_pred_all = rf_model.predict(X_sample)\n",
    "\n",
    "mask_alto  = (y_pred_all == clases.index(\"Alto\")) & (y_sample.values == clases.index(\"Alto\"))\n",
    "idx_caso1  = X_sample.index[mask_alto][0]\n",
    "empresa1   = X_sample.loc[idx_caso1]\n",
    "prob_caso1 = rf_model.predict_proba(empresa1.values.reshape(1, -1))[0]\n",
    "\n",
    "mask_bajo  = (y_pred_all == clases.index(\"Bajo\")) & (y_sample.values == clases.index(\"Bajo\"))\n",
    "idx_caso2  = X_sample.index[mask_bajo][0]\n",
    "empresa2   = X_sample.loc[idx_caso2]\n",
    "prob_caso2 = rf_model.predict_proba(empresa2.values.reshape(1, -1))[0]\n",
    "\n",
    "shap_raw   = explainer_rf.shap_values(X_sample)\n",
    "if isinstance(shap_raw, np.ndarray) and shap_raw.ndim == 3:\n",
    "    shap_vals = [shap_raw[:, :, i] for i in range(shap_raw.shape[2])]\n",
    "elif isinstance(shap_raw, list):\n",
    "    shap_vals = shap_raw\n",
    "else:\n",
    "    shap_vals = [shap_raw]\n",
    "\n",
    "shap_caso1 = shap_vals[clases.index(\"Alto\")][np.where(X_sample.index == idx_caso1)[0][0]]\n",
    "shap_caso2 = shap_vals[clases.index(\"Bajo\")][np.where(X_sample.index == idx_caso2)[0][0]]\n",
    "\n",
    "ev         = explainer_rf.expected_value\n",
    "base_alto  = ev[clases.index(\"Alto\")] if hasattr(ev, \"__len__\") else float(ev)\n",
    "base_bajo  = ev[clases.index(\"Bajo\")]  if hasattr(ev, \"__len__\") else float(ev)\n",
    "\n",
    "print(f\"  Caso 1 (Alto, idx={idx_caso1}): P={prob_caso1}\")\n",
    "print(f\"  Caso 2 (Bajo, idx={idx_caso2}): P={prob_caso2}\")\n",
    "print(f\"  Feature decisiva Caso 1: {feature_cols[np.argmax(np.abs(shap_caso1))]}\")\n",
    "print(f\"  Feature decisiva Caso 2: {feature_cols[np.argmax(np.abs(shap_caso2))]}\")\n",
    "print(\"\\n  Bloque 3 completado (imagenes 54-56).\")\n",
    "print(\"\\n=\" * 30)\n",
    "print(\"  NOTEBOOK COMPLETADO — Scripts 7, 8 y 9 ejecutados.\")\n",
    "print(\"=\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
